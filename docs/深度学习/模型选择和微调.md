# 模型选择与微调

## Resnet网络

### 主要特点

1. 残差连接:ResNet引入了残差连接(skip connections),允许网络直接学习残差映射。
2. 深度训练:通过残差连接,可以训练极深的网络(如100层、152层)。
3. 减少梯度消失:残差连接帮助缓解梯度消失问题。
4. 提高性能:ResNet在ImageNet分类任务中表现出色,连续几届ILSVRC比赛获奖。

### ResNet版本

1. ResNet v1:原始版本,使用预激活和批归一化。
2. ResNet v2:改进版,去除了预激活,增加了批归一化。

## Resnet不同层数

**Resnet18/Resnet34/Resnet50等**

## VGG网络

VGG网络(Very Deep Convolutional Networks)是由Oxford大学的Visual Geometry Group开发的深度卷积神经网络(CNN)架构。它是为了增加CNN的深度,从而提高模型性能而创建的。

### 主要特点

1. 果深:VGG网络非常深,有16层或19层。
2. 小卷积核:使用3x3的卷积核,这是VGG的核心特征。
3. ReLU激活:采用ReLU激活函数,大大减少了训练时间。
4. 全局平均池化:在最后的全连接层前添加全局平均池化层。
5. 简单结构:网络结构简单,易于理解和实现。

## VGG16和VGG19

1. VGG16:16层,约138万参数。
2. VGG19:19层,约150万参数。

### 网络结构

1. 输入:224x224的图像。
2. 卷积层:13层,每两层相邻的卷积层之间插入一个最大池化层。
3. 全连接层:三个全连接层,分别有4096、4096和1000个神经元

### 限制

1. 计算成本高:训练时间长。
2. 参数量大:模型较大。

## 迁移学习/模型微调

### 定义

迁移学习是一种机器学习技术,其中在一个任务上预训练的模型被微调以适应新的相关任务。这种方法允许我们利用在一个域上获得的知识来提高在另一个相关域上的性能。

### 主要特点

- 加速训练过程:通过使用预训练模型,我们可以快速开始新任务的训练,而不是从头开始训练整个模型。
- 数据需求降低:对于新任务,我们可以使用比完全从头开始训练所需的更少的数据。
- 计算资源优化:由于只需微调部分模型,因此计算资源需求大大降低。
- 性能提升:预训练模型通常具有更好的泛化能力,能够更好地适应新任务。

### 策略

- 直接微调:调整所有可训练层的权重。
- 冻结部分层:冻结某些预训练层的权重,仅微调新添加的层。
- 异质迁移:在源和目标域之间有一定的差异。
- 同质迁移:源和目标域相同,但任务不同。
- 无监督迁移:仅使用无标记数据进行迁移。

### 步骤

1. 选择预训练模型
2. 配置预训练模型
   - 冻结预训练层
   - 添加新层
3. 微调模型
4. 调整超参数
5. 评估性能

### 优势

1. 增加效率:减少训练时间和资源需求。
2. 提高性能:利用预训练模型的通用知识。
3. 增加可访问性:使深度学习技术更具可行性。

**代码举例**

保存网络中提取特征层，替换或增加最后的全连接层减少训练时间

```python
class CustomResNet18(nn.Module):
    def __init__(self, num_classes=4, dropout_rate=0.5):
        super(CustomResNet18, self).__init__()
        self.resnet18 = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)

        # # 冻结预训练模型的所有卷积层
        # for name, param in self.resnet18.named_parameters():
        #     if "fc" not in name:  # 只冻结卷积层
        #         param.requires_grad = False

        # 替换最后的全连接层
        self.resnet18.fc = nn.Sequential(
            nn.Flatten(),
            nn.BatchNorm1d(512),  # 添加BatchNorm1d层
            nn.Linear(512, 16),  # 添加全连接层
            nn.ReLU(),  # 添加ReLU激活函数
            nn.Dropout(dropout_rate),  # 添加Dropout层
            nn.BatchNorm1d(16),     # 添加BatchNorm1d层
            nn.Linear(16, num_classes),
            nn.Softmax(dim=1)
        )

    def forward(self, x):
        return self.resnet18(x)
```

